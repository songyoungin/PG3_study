# Generate MNIST images

import torch
import torch.nn as nn
from torch.optim import Adam
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchvision.utils import save_image


import numpy as np
import os

from vis_tool import Visualizer

# Visdom setting
vis = Visualizer()

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Image processing
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels
                         std=(0.5, 0.5, 0.5))])

# Hyper parameters
latent_size = 64
hidden_size = 256
image_size = 784
n_epochs = 100
batch_size = 100
sample_dir = 'samples'

# Create a directory if not exists
if not os.path.exists(sample_dir):
    os.makedirs(sample_dir)

# Set dataset and dataloader
mnist = datasets.MNIST(root='../data',
                       download=True,
                       transform=transform)

dataloader = DataLoader(mnist, batch_size=100, shuffle=True)

# Generator:
# Input: random vector z ~ uniform dist. or normal dist.
# Output: fake image
# Simple distribution --- [Generator] ---> Complex distribution

# Hyper parameter: latent size
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(latent_size, hidden_size),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_size, hidden_size*2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_size*2, hidden_size*4),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_size*4, image_size), # MNIST output size
            nn.Tanh()
        )

    # return random fake MNIST image
    def forward(self, x):
        return self.net(x).view(-1, 1, 28, 28) # batch, ch, h, w


# Discriminator:
# Input: a image
# Output: probability that input image is real image (0~1)

# Activation function: Sigmoid
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(image_size, hidden_size*4),
            nn.LeakyReLU(0.2),
            nn.Dropout(),
            nn.Linear(hidden_size*4, hidden_size*2),
            nn.LeakyReLU(0.2),
            nn.Dropout(),
            nn.Linear(hidden_size*2, hidden_size),
            nn.LeakyReLU(0.2),
            nn.Dropout(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid() # probability
        )

    def forward(self, x):
        x = x.view(-1, 28*28) # flatten => batch, image_size
        return self.net(x)

def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)


if __name__ == "__main__":
    G = Generator().to(device)
    D = Discriminator().to(device)

    # load weights trained G and D
    G = torch.load("weights/gan_generator.pth")
    D = torch.load("weights/gan_discriminator.pth")

    # set loss function and optimizers
    criterion = nn.BCELoss()

    G_opt = Adam(G.parameters(), lr=0.0002)
    D_opt = Adam(D.parameters(), lr=0.0002)

    # reset gradient of optimizers
    def reset_grad():
        G_opt.zero_grad()
        D_opt.zero_grad()

    total_step = len(dataloader)
    for epoch in range(n_epochs):
        avg_D_loss = []
        avg_G_loss = []

        for step, (real_data, _) in enumerate(dataloader):
            real_data = real_data.to(device) # shape: batch, 1, 28, 28

            # create the labels
            target_real = torch.ones(batch_size, 1).to(device)
            target_fake = torch.zeros(batch_size, 1).to(device)

            # ================================================================== #
            #                      Train the discriminator                       #
            # ================================================================== #

            # D(real image) => must be ~ 1
            # D(fake image) => must be ~ 0
            # loss = abs(D(real image)-1) + abs(D(fake image)-0)
            # minimize this loss

            # compute loss using real images
            D_result_from_real = D(real_data)
            D_loss_real = criterion(D_result_from_real, target_real)
            D_score_real = D_result_from_real

            # compute loss using fake images generated by G
            z = torch.randn(batch_size, latent_size).to(device)
            fake_data = G(z)
            D_result_from_fake = D(fake_data)
            D_loss_fake = criterion(D_result_from_fake, target_fake)
            D_score_fake = D_result_from_fake

            # loss + forward + backward
            D_loss = D_loss_real + D_loss_fake
            reset_grad()
            D_loss.backward()
            D_opt.step()

            # ================================================================== #
            #                        Train the generator                         #
            # ================================================================== #

            # compute loss using fake images generated by G
            z = torch.randn(batch_size, latent_size).to(device)
            fake_data = G(z)
            D_result_from_fake = D(fake_data)

            # loss + forward + backward
            G_loss = criterion(D_result_from_fake, target_real)
            reset_grad()
            G_loss.backward()
            G_opt.step()


            if (step+1) % 100 == 0:
                print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'
                      .format(epoch+1+100, n_epochs+100,
                              step + 1, total_step,
                              D_loss.item(), G_loss.item(),
                              D_score_real.mean().item(),
                              D_score_fake.mean().item()))

            avg_D_loss.append(D_loss.item())
            avg_G_loss.append(G_loss.item())

        avg_D_loss = np.mean(avg_D_loss)
        avg_G_loss = np.mean(avg_G_loss)

        true_positive_rate = (D_result_from_real > 0.5).float().mean().item()
        true_negative_rate = (D_result_from_fake < 0.5).float().mean().item()

        base_message = ("Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} "
                        "True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}"
                        )

        message = base_message.format(
            epoch=100 + epoch + 1,
            d_loss=avg_D_loss,
            g_loss=avg_G_loss,
            tpr=true_positive_rate,
            tnr=true_negative_rate
        )
        print(message) # pring logging

        vis.plot("Discriminator Loss per epoch", avg_D_loss)
        vis.plot("Generator Loss per epoch", avg_G_loss)

        # Save real images
        if (epoch + 1) == 1:
            save_image(denorm(real_data), os.path.join(sample_dir, 'real_images.png'))

        # Save sampled images
        save_image(denorm(fake_data), os.path.join(sample_dir, 'fake_images-{}.png'.format(100+ epoch + 1)))

    torch.save(G, "weights/G_ep200.pth")
    torch.save(D, "weights/D_ep200.pth")



