from dataloader import get_loader
from model import Generator, Discriminator
from vis_tool import Visualizer

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.utils import save_image

import os
import numpy as np

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

### Before begining training, you must compute weight decay epochs ###

class Trainer():
    # initializer
    def __init__(self, root, batch_size, lr, num_epochs):
        # define dataloader
        self.dataloader = get_loader(root, batch_size)

        # define loss function
        self.criterion = nn.BCELoss()

        self.lr = lr
        self.num_epochs = num_epochs

        self.build_model()
        self.build_opts()

        # visualize tool
        self.vis = Visualizer()

    # build model and initialize
    def build_model(self):
        # define generator, discriminator
        self.g = Generator().to(device)
        self.d = Discriminator().to(device)

        # initialize model's weight
        self.g.weight_init(mean=0.0, std=0.02)
        self.d.weight_init(mean=0.0, std=0.02)

    # define optimizers
    def build_opts(self):
        self.g_opt = optim.Adam(self.g.parameters(), self.lr, betas=(0.5, 0.999))
        self.d_opt = optim.Adam(self.d.parameters(), self.lr, betas=(0.5, 0.999))

    # for saving result images
    def denorm(self, x):
        out = (x + 1) / 2
        return out.clamp(0, 1)

    # reset opts' gradients
    def reset_grad(self):
        self.g_opt.zero_grad()
        self.d_opt.zero_grad()

    # training
    def train(self):
        sample_dir = 'samples'

        # Create a directory if not exists
        if not os.path.exists(sample_dir):
            os.makedirs(sample_dir)

        total_step = len(self.dataloader)
        print("total step:", total_step)
        smallest_loss = 50000.0

        for epoch in range(self.num_epochs):
            avg_D_loss = []
            avg_G_loss = []

            # decay learning rate
            if (epoch+1) == 11:
                self.g_opt.param_groups[0]['lr'] /= 10
                self.d_opt.param_groups[0]['lr'] /= 10
                print("learning rate decay!")

            if (epoch+1) == 16:
                self.g_opt.param_groups[0]['lr'] /= 10
                self.d_opt.param_groups[0]['lr'] /= 10
                print("learning rate decay!")

            for step, (real_data, _) in enumerate(self.dataloader):
                real_data = real_data.to(device)  # shape: batch, 1, 28, 28
                # print("read_data shape:", real_data.shape)

                step_batch = real_data.size()[0] # batch size of this step

                # create the labels
                target_real = torch.ones(step_batch).to(device)
                # print("target_real shape:", target_real.shape)

                target_fake = torch.zeros(step_batch).to(device)
                # print("target_fake shape:", target_fake.shape)

                # ================================================================== #
                #                      Train the discriminator                       #
                # ================================================================== #

                # D(real image) => must be ~ 1
                # D(fake image) => must be ~ 0
                # loss = abs(D(real image)-1) + abs(D(fake image)-0)
                # minimize this loss

                # compute loss using real images
                D_result_from_real = self.d(real_data)
                D_loss_real = self.criterion(D_result_from_real, target_real)
                D_score_real = D_result_from_real

                # compute loss using fake images generated by G
                z = torch.randn(step_batch, 100).to(device)
                z = z.view(-1, 100, 1, 1)
                # print("random vector Z shape:", z.shape)

                fake_data = self.g(z)
                # print("fake_data shape:", fake_data.shape)

                D_result_from_fake = self.d(fake_data)
                D_loss_fake = self.criterion(D_result_from_fake, target_fake)
                D_score_fake = D_result_from_fake

                # loss + forward + backward
                D_loss = D_loss_real + D_loss_fake
                self.reset_grad()
                D_loss.backward()
                self.d_opt.step()

                # ================================================================== #
                #                        Train the generator                         #
                # ================================================================== #

                # compute loss using fake images generated by G
                z = torch.randn(step_batch, 100).to(device)
                z = z.view(-1, 100, 1, 1)

                fake_data = self.g(z)
                D_result_from_fake = self.d(fake_data)

                # loss + forward + backward
                G_loss = self.criterion(D_result_from_fake, target_real)

                self.reset_grad()
                G_loss.backward()
                self.g_opt.step()

                if (step + 1) % 10 == 0:
                    print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'
                          .format(epoch + 1, self.num_epochs,
                                  step + 1, total_step,
                                  D_loss.item(), G_loss.item(),
                                  D_score_real.mean().item(),
                                  D_score_fake.mean().item()))

                    # plot to visdom
                    self.vis.plot("Discriminator Loss per step", D_loss.item())
                    self.vis.plot("Generator Loss per step", G_loss.item())

                avg_D_loss.append(D_loss.item())
                avg_G_loss.append(G_loss.item())

            avg_D_loss = np.mean(avg_D_loss)
            avg_G_loss = np.mean(avg_G_loss)

            true_positive_rate = (D_result_from_real > 0.5).float().mean().item()
            true_negative_rate = (D_result_from_fake < 0.5).float().mean().item()

            base_message = ("Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} "
                            "True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}"
                            )

            message = base_message.format(
                epoch=epoch + 1,
                d_loss=avg_D_loss,
                g_loss=avg_G_loss,
                tpr=true_positive_rate,
                tnr=true_negative_rate
            )
            print(message)  # pring logging

            # plot to visdom
            self.vis.plot("Discriminator Loss per epoch", avg_D_loss)
            self.vis.plot("Generator Loss per epoch", avg_G_loss)

            # Save real images
            if (epoch + 1) == 1:
                save_image(self.denorm(real_data), os.path.join(sample_dir, 'real_images.png'))

            # Save sampled images
            save_image(self.denorm(fake_data), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch + 1)))
            print("Result save complete!")

            # save least lost model
            avg_loss = (avg_D_loss + avg_G_loss) / float(2)
            if avg_loss < smallest_loss:
                smallest_loss = avg_loss
                torch.save(self.g, "weights/temp_best_G.pth")
                torch.save(self.d, "weights/temp_best_D.pth")

        print("learning complete!!")
        torch.save(self.g, "weights/DCGAN_celebA_G_ep"+str(self.num_epochs)+".pth")
        torch.save(self.d, "weights/DCGAN_celebA_D_ep"+str(self.num_epochs)+".pth")
        print("Weight save complete!")

if __name__ == "__main__":
    root = '../../data/resized_celebA/'
    batch_size = 128
    lr = 0.002
    num_epochs = 20

    trainer = Trainer(root, batch_size, lr, num_epochs)
    trainer.train()